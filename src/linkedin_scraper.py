"""
LinkedIn Scraper –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ SaaS –∏–¥–µ–π

–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø–æ—Å—Ç—ã, –∫–æ–º–ø–∞–Ω–∏–∏ –∏ –ø—Ä–æ—Ñ–∏–ª–∏ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è B2B —Ä—ã–Ω–∫–∞
"""

from linkedin_api import Linkedin
import pandas as pd
from datetime import datetime, timedelta
import json
import re
from collections import Counter
import time


class LinkedInSaaSValidator:
    def __init__(self, email, password):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LinkedIn API –∫–ª–∏–µ–Ω—Ç–∞
        
        –í–ê–ñ–ù–û: –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –æ—Å—Ç–æ—Ä–æ–∂–Ω–æ! LinkedIn –º–æ–∂–µ—Ç –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å –∞–∫–∫–∞—É–Ω—Ç –∑–∞ scraping
        
        Args:
            email: Email LinkedIn –∞–∫–∫–∞—É–Ω—Ç–∞
            password: –ü–∞—Ä–æ–ª—å LinkedIn –∞–∫–∫–∞—É–Ω—Ç–∞
            
        –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:
        - –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –æ—Ç–¥–µ–ª—å–Ω—ã–π —Ç–µ—Å—Ç–æ–≤—ã–π –∞–∫–∫–∞—É–Ω—Ç
        - –î–æ–±–∞–≤–ª—è–π—Ç–µ –∑–∞–¥–µ—Ä–∂–∫–∏ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
        - –ù–µ –¥–µ–ª–∞–π—Ç–µ —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤ –∑–∞ —Ä–∞–∑
        """
        try:
            self.api = Linkedin(email, password)
            print("‚úÖ LinkedIn API –ø–æ–¥–∫–ª—é—á–µ–Ω")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ LinkedIn: {e}")
            raise
    
    def search_posts(self, keywords, limit=50):
        """
        –ü–æ–∏—Å–∫ –ø–æ—Å—Ç–æ–≤ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        
        LinkedIn API –æ–≥—Ä–∞–Ω–∏—á–µ–Ω, –ø–æ—ç—Ç–æ–º—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –º–æ–≥—É—Ç –±—ã—Ç—å –Ω–µ–ø–æ–ª–Ω—ã–º–∏
        
        Args:
            keywords: —Å–ø–∏—Å–æ–∫ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            limit: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Å—Ç–æ–≤
            
        Returns:
            DataFrame —Å –ø–æ—Å—Ç–∞–º–∏
        """
        posts_data = []
        
        print(f"–ü–æ–∏—Å–∫ –ø–æ—Å—Ç–æ–≤ LinkedIn –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º: {keywords}")
        
        try:
            # –ü–æ–∏—Å–∫ –ø–æ –∫–∞–∂–¥–æ–º—É –∫–ª—é—á–µ–≤–æ–º—É —Å–ª–æ–≤—É
            for keyword in keywords:
                print(f"  –ü–æ–∏—Å–∫: '{keyword}'")
                
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–∏—Å–∫ –∫–æ–º–ø–∞–Ω–∏–π, —Ç–∞–∫ –∫–∞–∫ –ø—Ä—è–º–æ–π –ø–æ–∏—Å–∫ –ø–æ—Å—Ç–æ–≤ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω
                results = self.api.search({'keywords': keyword}, limit=limit)
                
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (–º–æ–∂–µ—Ç –ø–æ—Ç—Ä–µ–±–æ–≤–∞—Ç—å—Å—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è)
                # LinkedIn API –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–∞–∑–Ω—ã–µ —Ç–∏–ø—ã –æ–±—ä–µ–∫—Ç–æ–≤
                
                time.sleep(2)  # Rate limiting
            
            print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(posts_data)} –ø–æ—Å—Ç–æ–≤")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
            return pd.DataFrame()
        
        return pd.DataFrame(posts_data)
    
    def get_company_info(self, company_name):
        """
        –ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–æ–º–ø–∞–Ω–∏–∏
        
        –ü–æ–ª–µ–∑–Ω–æ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤
        """
        print(f"–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–æ–º–ø–∞–Ω–∏–∏: {company_name}")
        
        try:
            # –ü–æ–∏—Å–∫ –∫–æ–º–ø–∞–Ω–∏–∏
            companies = self.api.search_companies(keywords=company_name, limit=5)
            
            if not companies:
                print(f"‚ùå –ö–æ–º–ø–∞–Ω–∏—è '{company_name}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
                return None
            
            # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            company_urn = companies[0].get('urn_id')
            
            # –ü–æ–ª—É—á–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
            company_data = self.api.get_company(company_urn)
            
            info = {
                'name': company_data.get('name'),
                'description': company_data.get('description'),
                'industry': company_data.get('companyIndustries', [{}])[0].get('localizedName'),
                'company_size': company_data.get('staffCount'),
                'followers': company_data.get('followersCount'),
                'website': company_data.get('companyPageUrl'),
                'founded': company_data.get('foundedOn', {}).get('year'),
                'headquarters': company_data.get('headquarter', {}).get('city'),
                'specialties': company_data.get('specialities', [])
            }
            
            print(f"‚úÖ –ü–æ–ª—É—á–µ–Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ {company_name}")
            return info
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
            return None
    
    def get_company_updates(self, company_urn, limit=20):
        """
        –ü–æ–ª—É—á–∏—Ç—å –Ω–µ–¥–∞–≤–Ω–∏–µ –ø–æ—Å—Ç—ã –∫–æ–º–ø–∞–Ω–∏–∏
        
        –ü–æ–ª–µ–∑–Ω–æ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–Ω—Ç–µ–Ω—Ç-—Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤
        """
        print(f"–ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ—Å—Ç–æ–≤ –∫–æ–º–ø–∞–Ω–∏–∏ (URN: {company_urn})")
        
        updates_data = []
        
        try:
            updates = self.api.get_company_updates(company_urn, max_results=limit)
            
            for update in updates:
                # –ü–∞—Ä—Å–∏–Ω–≥ –¥–∞–Ω–Ω—ã—Ö –ø–æ—Å—Ç–∞
                update_info = {
                    'urn': update.get('urn'),
                    'text': update.get('commentary', {}).get('text', ''),
                    'created_at': datetime.fromtimestamp(update.get('created', {}).get('time', 0) / 1000),
                    'likes': update.get('socialDetail', {}).get('totalSocialActivityCounts', {}).get('numLikes', 0),
                    'comments': update.get('socialDetail', {}).get('totalSocialActivityCounts', {}).get('numComments', 0),
                    'shares': update.get('socialDetail', {}).get('totalSocialActivityCounts', {}).get('numShares', 0),
                }
                
                updates_data.append(update_info)
            
            print(f"‚úÖ –ü–æ–ª—É—á–µ–Ω–æ {len(updates_data)} –ø–æ—Å—Ç–æ–≤")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
            return pd.DataFrame()
        
        return pd.DataFrame(updates_data)
    
    def search_people(self, keywords, industry=None, limit=50):
        """
        –ü–æ–∏—Å–∫ –ª—é–¥–µ–π –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        
        –ü–æ–ª–µ–∑–Ω–æ –¥–ª—è –ø–æ–∏—Å–∫–∞ potential customers –∏–ª–∏ thought leaders
        
        Args:
            keywords: –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–¥–æ–ª–∂–Ω–æ—Å—Ç–∏, –Ω–∞–≤—ã–∫–∏)
            industry: –∫–æ–¥ –∏–Ω–¥—É—Å—Ç—Ä–∏–∏ LinkedIn (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            limit: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        """
        print(f"–ü–æ–∏—Å–∫ –ª—é–¥–µ–π: '{keywords}'")
        
        people_data = []
        
        try:
            search_params = {'keywords': keywords}
            
            if industry:
                search_params['industry'] = industry
            
            results = self.api.search_people(
                **search_params,
                limit=limit
            )
            
            for person in results:
                people_data.append({
                    'name': f"{person.get('firstName', '')} {person.get('lastName', '')}",
                    'headline': person.get('headline', ''),
                    'location': person.get('location', ''),
                    'industry': person.get('industry', ''),
                    'profile_url': f"https://www.linkedin.com/in/{person.get('public_id', '')}"
                })
            
            print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(people_data)} –ª—é–¥–µ–π")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
            return pd.DataFrame()
        
        return pd.DataFrame(people_data)
    
    def get_profile(self, public_id):
        """
        –ü–æ–ª—É—á–∏—Ç—å –¥–µ—Ç–∞–ª—å–Ω—ã–π –ø—Ä–æ—Ñ–∏–ª—å —á–µ–ª–æ–≤–µ–∫–∞
        
        Args:
            public_id: LinkedIn public ID (–∏–∑ URL –ø—Ä–æ—Ñ–∏–ª—è)
        """
        print(f"–ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–æ—Ñ–∏–ª—è: {public_id}")
        
        try:
            profile = self.api.get_profile(public_id)
            
            profile_data = {
                'name': f"{profile.get('firstName', '')} {profile.get('lastName', '')}",
                'headline': profile.get('headline', ''),
                'summary': profile.get('summary', ''),
                'location': profile.get('locationName', ''),
                'industry': profile.get('industryName', ''),
                'connections': profile.get('connectionsCount', 0),
                'followers': profile.get('followersCount', 0),
                'experience': [],
                'education': []
            }
            
            # –û–ø—ã—Ç —Ä–∞–±–æ—Ç—ã
            for exp in profile.get('experience', []):
                profile_data['experience'].append({
                    'title': exp.get('title'),
                    'company': exp.get('companyName'),
                    'location': exp.get('locationName'),
                    'start': exp.get('timePeriod', {}).get('startDate'),
                    'end': exp.get('timePeriod', {}).get('endDate')
                })
            
            # –û–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ
            for edu in profile.get('education', []):
                profile_data['education'].append({
                    'school': edu.get('schoolName'),
                    'degree': edu.get('degreeName'),
                    'field': edu.get('fieldOfStudy'),
                    'start': edu.get('timePeriod', {}).get('startDate'),
                    'end': edu.get('timePeriod', {}).get('endDate')
                })
            
            print(f"‚úÖ –ü–æ–ª—É—á–µ–Ω –ø—Ä–æ—Ñ–∏–ª—å: {profile_data['name']}")
            return profile_data
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
            return None
    
    def analyze_competitors(self, competitor_names, output_file='linkedin_competitors.json'):
        """
        –ê–Ω–∞–ª–∏–∑ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ –Ω–∞ LinkedIn
        
        –°–æ–±–∏—Ä–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–æ–º–ø–∞–Ω–∏—è—Ö –∏ –∏—Ö –∫–æ–Ω—Ç–µ–Ω—Ç-—Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
        """
        print(f"\n{'='*60}")
        print(f"LinkedIn –ê–Ω–∞–ª–∏–∑ –ö–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤")
        print(f"{'='*60}\n")
        
        competitors_data = []
        
        for competitor in competitor_names:
            print(f"\nüìä –ê–Ω–∞–ª–∏–∑: {competitor}")
            print(f"{'-'*40}")
            
            # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–æ–º–ø–∞–Ω–∏–∏
            company_info = self.get_company_info(competitor)
            
            if company_info:
                competitor_data = {
                    'company': competitor,
                    'info': company_info,
                    'posts': [],
                    'engagement': {}
                }
                
                # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å—Ç—ã –∫–æ–º–ø–∞–Ω–∏–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å URN)
                # –≠—Ç–æ —Ç—Ä–µ–±—É–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –∫–æ–º–ø–∞–Ω–∏–∏ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è URN
                try:
                    companies = self.api.search_companies(keywords=competitor, limit=1)
                    if companies:
                        company_urn = companies[0].get('urn_id')
                        posts_df = self.get_company_updates(company_urn, limit=20)
                        
                        if not posts_df.empty:
                            competitor_data['posts'] = posts_df.to_dict('records')
                            competitor_data['engagement'] = {
                                'avg_likes': float(posts_df['likes'].mean()),
                                'avg_comments': float(posts_df['comments'].mean()),
                                'avg_shares': float(posts_df['shares'].mean()),
                                'total_posts': len(posts_df)
                            }
                            
                            print(f"  üìà –ü–æ—Å—Ç—ã: {len(posts_df)}")
                            print(f"  ‚ù§Ô∏è Avg likes: {competitor_data['engagement']['avg_likes']:.1f}")
                            print(f"  üí¨ Avg comments: {competitor_data['engagement']['avg_comments']:.1f}")
                
                except Exception as e:
                    print(f"  ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –ø–æ—Å—Ç—ã: {e}")
                
                competitors_data.append(competitor_data)
                
                # Rate limiting
                time.sleep(3)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        results = {
            'analysis_date': datetime.now().isoformat(),
            'competitors': competitors_data
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"\n‚úÖ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_file}")
        
        return results
    
    def find_target_audience(self, job_titles, industries=None, locations=None, limit=100):
        """
        –ü–æ–∏—Å–∫ —Ü–µ–ª–µ–≤–æ–π –∞—É–¥–∏—Ç–æ—Ä–∏–∏ –¥–ª—è SaaS –ø—Ä–æ–¥—É–∫—Ç–∞
        
        Args:
            job_titles: —Å–ø–∏—Å–æ–∫ –¥–æ–ª–∂–Ω–æ—Å—Ç–µ–π (–Ω–∞–ø—Ä–∏–º–µ—Ä, ['Marketing Manager', 'CMO'])
            industries: —Å–ø–∏—Å–æ–∫ –∏–Ω–¥—É—Å—Ç—Ä–∏–π (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            locations: —Å–ø–∏—Å–æ–∫ –ª–æ–∫–∞—Ü–∏–π (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            limit: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            
        Returns:
            DataFrame —Å potential customers
        """
        print(f"\n{'='*60}")
        print(f"–ü–æ–∏—Å–∫ —Ü–µ–ª–µ–≤–æ–π –∞—É–¥–∏—Ç–æ—Ä–∏–∏")
        print(f"{'='*60}\n")
        
        all_people = []
        
        for job_title in job_titles:
            print(f"üîç –ü–æ–∏—Å–∫: {job_title}")
            
            people_df = self.search_people(
                keywords=job_title,
                limit=limit
            )
            
            if not people_df.empty:
                people_df['search_job_title'] = job_title
                all_people.append(people_df)
            
            # Rate limiting
            time.sleep(2)
        
        if not all_people:
            print("‚ùå –¶–µ–ª–µ–≤–∞—è –∞—É–¥–∏—Ç–æ—Ä–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            return pd.DataFrame()
        
        combined = pd.concat(all_people, ignore_index=True)
        combined = combined.drop_duplicates(subset=['name'])
        
        print(f"\n‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(combined)} potential customers")
        
        # –ê–Ω–∞–ª–∏–∑ –ø–æ –∏–Ω–¥—É—Å—Ç—Ä–∏—è–º
        if not combined.empty:
            print(f"\nüìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∏–Ω–¥—É—Å—Ç—Ä–∏—è–º:")
            industry_counts = combined['industry'].value_counts().head(10)
            for industry, count in industry_counts.items():
                print(f"  {industry}: {count}")
        
        return combined
    
    def validate_b2b_market(self, 
                           target_job_titles,
                           competitor_names,
                           product_keywords,
                           output_file='linkedin_b2b_validation.json'):
        """
        –ü–æ–ª–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è B2B —Ä—ã–Ω–∫–∞ —á–µ—Ä–µ–∑ LinkedIn
        
        Args:
            target_job_titles: —Å–ø–∏—Å–æ–∫ —Ü–µ–ª–µ–≤—ã—Ö –¥–æ–ª–∂–Ω–æ—Å—Ç–µ–π
            competitor_names: —Å–ø–∏—Å–æ–∫ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤
            product_keywords: –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –ø—Ä–æ–¥—É–∫—Ç–∞
            output_file: —Ñ–∞–π–ª –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        """
        print(f"\n{'='*60}")
        print(f"LinkedIn B2B –í–∞–ª–∏–¥–∞—Ü–∏—è")
        print(f"{'='*60}\n")
        
        validation_results = {
            'analysis_date': datetime.now().isoformat(),
            'target_job_titles': target_job_titles,
            'competitors_analyzed': competitor_names,
            'product_keywords': product_keywords,
            'market_size': 0,
            'competitors_data': [],
            'audience_insights': {},
            'validation_score': 0,
            'verdict': ''
        }
        
        # 1. –ü–æ–∏—Å–∫ —Ü–µ–ª–µ–≤–æ–π –∞—É–¥–∏—Ç–æ—Ä–∏–∏
        print("\nüë• –®–∞–≥ 1: –ê–Ω–∞–ª–∏–∑ —Ü–µ–ª–µ–≤–æ–π –∞—É–¥–∏—Ç–æ—Ä–∏–∏")
        audience_df = self.find_target_audience(
            job_titles=target_job_titles,
            limit=100
        )
        
        if not audience_df.empty:
            validation_results['market_size'] = len(audience_df)
            validation_results['audience_insights'] = {
                'total_found': len(audience_df),
                'top_industries': audience_df['industry'].value_counts().head(10).to_dict(),
                'top_locations': audience_df['location'].value_counts().head(10).to_dict()
            }
        
        # 2. –ê–Ω–∞–ª–∏–∑ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤
        if competitor_names:
            print(f"\nüè¢ –®–∞–≥ 2: –ê–Ω–∞–ª–∏–∑ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤")
            competitors_analysis = self.analyze_competitors(
                competitor_names=competitor_names,
                output_file='linkedin_competitors_temp.json'
            )
            
            validation_results['competitors_data'] = competitors_analysis.get('competitors', [])
        
        # 3. Scoring
        score = 0
        reasons = []
        
        # –†–∞–∑–º–µ—Ä –∞—É–¥–∏—Ç–æ—Ä–∏–∏
        if validation_results['market_size'] > 500:
            score += 30
            reasons.append(f"‚úÖ –ë–æ–ª—å—à–∞—è —Ü–µ–ª–µ–≤–∞—è –∞—É–¥–∏—Ç–æ—Ä–∏—è ({validation_results['market_size']}+ –ø—Ä–æ—Ñ–∏–ª–µ–π)")
        elif validation_results['market_size'] > 200:
            score += 20
            reasons.append(f"‚úÖ –°—Ä–µ–¥–Ω—è—è —Ü–µ–ª–µ–≤–∞—è –∞—É–¥–∏—Ç–æ—Ä–∏—è ({validation_results['market_size']}+ –ø—Ä–æ—Ñ–∏–ª–µ–π)")
        elif validation_results['market_size'] > 50:
            score += 10
            reasons.append(f"‚ö†Ô∏è –ù–µ–±–æ–ª—å—à–∞—è —Ü–µ–ª–µ–≤–∞—è –∞—É–¥–∏—Ç–æ—Ä–∏—è ({validation_results['market_size']}+ –ø—Ä–æ—Ñ–∏–ª–µ–π)")
        
        # –ö–æ–Ω–∫—É—Ä–µ–Ω—Ç—ã
        if len(validation_results['competitors_data']) > 0:
            score += 20
            reasons.append(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(validation_results['competitors_data'])} –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤")
            
            # –ê–Ω–∞–ª–∏–∑ engagement –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤
            total_engagement = 0
            for comp in validation_results['competitors_data']:
                if comp.get('engagement'):
                    total_engagement += comp['engagement'].get('avg_likes', 0)
            
            if total_engagement > 100:
                score += 15
                reasons.append("‚úÖ –í—ã—Å–æ–∫–∏–π engagement —É –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ - –∞–∫—Ç–∏–≤–Ω—ã–π —Ä—ã–Ω–æ–∫")
        
        # –†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –∏–Ω–¥—É—Å—Ç—Ä–∏–π (–ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —à–∏—Ä–æ—Ç—É –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è)
        if validation_results.get('audience_insights', {}).get('top_industries'):
            num_industries = len(validation_results['audience_insights']['top_industries'])
            if num_industries > 5:
                score += 15
                reasons.append(f"‚úÖ –®–∏—Ä–æ–∫–∏–π –æ—Ö–≤–∞—Ç –∏–Ω–¥—É—Å—Ç—Ä–∏–π ({num_industries})")
            elif num_industries > 3:
                score += 10
                reasons.append(f"‚úÖ –£–º–µ—Ä–µ–Ω–Ω—ã–π –æ—Ö–≤–∞—Ç –∏–Ω–¥—É—Å—Ç—Ä–∏–π ({num_industries})")
        
        # –ü—Ä–∏—Å—É—Ç—Å—Ç–≤–∏–µ –≤ LinkedIn (–ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç B2B —Ñ–æ–∫—É—Å)
        score += 20  # Bonus –∑–∞ —Ç–æ, —á—Ç–æ –Ω–∞—à–ª–∏ –∞—É–¥–∏—Ç–æ—Ä–∏—é –≤ LinkedIn
        reasons.append("‚úÖ –¶–µ–ª–µ–≤–∞—è –∞—É–¥–∏—Ç–æ—Ä–∏—è –∞–∫—Ç–∏–≤–Ω–∞ –Ω–∞ LinkedIn")
        
        validation_results['validation_score'] = score
        validation_results['score_reasons'] = reasons
        
        # –í–µ—Ä–¥–∏–∫—Ç
        if score >= 80:
            verdict = "üöÄ –û–¢–õ–ò–ß–ù–´–ô B2B –†–´–ù–û–ö - –°–∏–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è"
        elif score >= 60:
            verdict = "‚úÖ –•–û–†–û–®–ò–ô B2B –†–´–ù–û–ö - –ï—Å—Ç—å –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª"
        elif score >= 40:
            verdict = "‚ö†Ô∏è –°–†–ï–î–ù–ò–ô B2B –†–´–ù–û–ö - –ù—É–∂–Ω—ã –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è"
        else:
            verdict = "‚ùå –°–õ–ê–ë–ê–Ø –í–ê–õ–ò–î–ê–¶–ò–Ø - –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è pivot"
        
        validation_results['verdict'] = verdict
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(validation_results, f, indent=2, ensure_ascii=False, default=str)
        
        # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        print(f"\n{'='*60}")
        print(f"üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ B2B –í–ê–õ–ò–î–ê–¶–ò–ò")
        print(f"{'='*60}")
        print(f"\nüéØ –û—Ü–µ–Ω–∫–∞: {validation_results['validation_score']}/100")
        print(f"üìã –í–µ—Ä–¥–∏–∫—Ç: {validation_results['verdict']}")
        print(f"\nüìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        print(f"  - –†–∞–∑–º–µ—Ä –∞—É–¥–∏—Ç–æ—Ä–∏–∏: {validation_results['market_size']} –ø—Ä–æ—Ñ–∏–ª–µ–π")
        print(f"  - –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤: {len(validation_results['competitors_data'])}")
        
        if reasons:
            print(f"\nüí° –ü–æ—á–µ–º—É —ç—Ç–∞ –æ—Ü–µ–Ω–∫–∞:")
            for reason in reasons:
                print(f"  {reason}")
        
        print(f"\n‚úÖ –ü–æ–ª–Ω—ã–π –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_file}")
        
        return validation_results


class LinkedInIndustries:
    """
    –ö–æ–¥—ã –∏–Ω–¥—É—Å—Ç—Ä–∏–π LinkedIn –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø–æ–∏—Å–∫–∞
    """
    
    # –û—Å–Ω–æ–≤–Ω—ã–µ –∏–Ω–¥—É—Å—Ç—Ä–∏–∏ –¥–ª—è B2B SaaS
    INDUSTRIES = {
        'software': '4',
        'it_services': '96',
        'internet': '6',
        'marketing': '80',
        'financial_services': '43',
        'management_consulting': '11',
        'health_tech': '14',
        'education': '69',
        'retail': '27',
        'real_estate': '44'
    }
    
    @classmethod
    def get_industry_code(cls, industry_name):
        """–ü–æ–ª—É—á–∏—Ç—å –∫–æ–¥ –∏–Ω–¥—É—Å—Ç—Ä–∏–∏ –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é"""
        return cls.INDUSTRIES.get(industry_name.lower())
    
    @classmethod
    def get_all_codes(cls):
        """–ü–æ–ª—É—á–∏—Ç—å –≤—Å–µ –∫–æ–¥—ã –∏–Ω–¥—É—Å—Ç—Ä–∏–π"""
        return list(cls.INDUSTRIES.values())


def main():
    """
    –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è LinkedIn scraper
    """
    
    # –í–ê–ñ–ù–û: –ó–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ –≤–∞—à–∏ credentials
    # –†–ï–ö–û–ú–ï–ù–î–£–ï–¢–°–Ø: –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –æ—Ç–¥–µ–ª—å–Ω—ã–π —Ç–µ—Å—Ç–æ–≤—ã–π –∞–∫–∫–∞—É–Ω—Ç!
    EMAIL = "–≤–∞—à_email@example.com"
    PASSWORD = "–≤–∞—à_–ø–∞—Ä–æ–ª—å"
    
    scraper = LinkedInSaaSValidator(
        email=EMAIL,
        password=PASSWORD
    )
    
    # –ü—Ä–∏–º–µ—Ä 1: –ü–æ–∏—Å–∫ —Ü–µ–ª–µ–≤–æ–π –∞—É–¥–∏—Ç–æ—Ä–∏–∏
    print("=" * 60)
    print("–ü–†–ò–ú–ï–† 1: –ü–æ–∏—Å–∫ —Ü–µ–ª–µ–≤–æ–π –∞—É–¥–∏—Ç–æ—Ä–∏–∏")
    print("=" * 60)
    
    target_audience = scraper.find_target_audience(
        job_titles=['Marketing Manager', 'CMO', 'Head of Marketing'],
        limit=50
    )
    
    if not target_audience.empty:
        target_audience.to_csv('linkedin_target_audience.csv', index=False)
        print(f"\n‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ linkedin_target_audience.csv")
    
    # –ü—Ä–∏–º–µ—Ä 2: –ê–Ω–∞–ª–∏–∑ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤
    print("\n" + "=" * 60)
    print("–ü–†–ò–ú–ï–† 2: –ê–Ω–∞–ª–∏–∑ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤")
    print("=" * 60)
    
    competitors = ['HubSpot', 'Mailchimp', 'Salesforce']
    
    competitors_analysis = scraper.analyze_competitors(
        competitor_names=competitors,
        output_file='linkedin_competitors.json'
    )
    
    # –ü—Ä–∏–º–µ—Ä 3: –ü–æ–ª–Ω–∞—è B2B –≤–∞–ª–∏–¥–∞—Ü–∏—è
    print("\n" + "=" * 60)
    print("–ü–†–ò–ú–ï–† 3: –ü–æ–ª–Ω–∞—è B2B –≤–∞–ª–∏–¥–∞—Ü–∏—è")
    print("=" * 60)
    
    validation = scraper.validate_b2b_market(
        target_job_titles=[
            'Marketing Director',
            'VP Marketing',
            'Growth Manager',
            'Marketing Operations'
        ],
        competitor_names=['HubSpot', 'Marketo', 'ActiveCampaign'],
        product_keywords=['marketing automation', 'email marketing', 'lead generation'],
        output_file='linkedin_b2b_validation.json'
    )
    
    # –ü—Ä–∏–º–µ—Ä 4: –ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–æ–º–ø–∞–Ω–∏–∏
    print("\n" + "=" * 60)
    print("–ü–†–ò–ú–ï–† 4: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫–æ–º–ø–∞–Ω–∏–∏")
    print("=" * 60)
    
    company_info = scraper.get_company_info('Notion')
    
    if company_info:
        print(f"\nüìä {company_info['name']}")
        print(f"   Industry: {company_info.get('industry', 'N/A')}")
        print(f"   Size: {company_info.get('company_size', 'N/A')} employees")
        print(f"   Followers: {company_info.get('followers', 'N/A'):,}")
        print(f"   Website: {company_info.get('website', 'N/A')}")


if __name__ == "__main__":
    main()
