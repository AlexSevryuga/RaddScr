"""
Reddit Scraper –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ SaaS –∏–¥–µ–π

–°–æ–±–∏—Ä–∞–µ—Ç –ø–æ—Å—Ç—ã –∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏, –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç engagement –∏ –Ω–∞—Ö–æ–¥–∏—Ç –ø—Ä–æ–±–ª–µ–º—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
"""

import praw
import pandas as pd
from datetime import datetime, timedelta
import json
import re
from collections import Counter
import time


class RedditSaaSValidator:
    def __init__(self, client_id, client_secret, user_agent):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Reddit API –∫–ª–∏–µ–Ω—Ç–∞
        
        –î–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è credentials:
        1. –ü–µ—Ä–µ–π–¥–∏—Ç–µ –Ω–∞ https://www.reddit.com/prefs/apps
        2. –°–æ–∑–¥–∞–π—Ç–µ –Ω–æ–≤–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ (script type)
        3. –ü–æ–ª—É—á–∏—Ç–µ client_id –∏ client_secret
        
        Args:
            client_id: ID –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è (–ø–æ–¥ "personal use script")
            client_secret: Secret –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
            user_agent: –û–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, "SaaS Validator by u/yourname")
        """
        self.reddit = praw.Reddit(
            client_id=client_id,
            client_secret=client_secret,
            user_agent=user_agent
        )
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è
        try:
            self.reddit.user.me()
            print("‚úÖ Reddit API –ø–æ–¥–∫–ª—é—á–µ–Ω (read-only mode)")
        except:
            print("‚úÖ Reddit API –ø–æ–¥–∫–ª—é—á–µ–Ω (anonymous mode)")
    
    def search_subreddit(self, subreddit_name, query, limit=100, time_filter='month', sort='relevance'):
        """
        –ü–æ–∏—Å–∫ –ø–æ—Å—Ç–æ–≤ –≤ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º subreddit
        
        Args:
            subreddit_name: –Ω–∞–∑–≤–∞–Ω–∏–µ subreddit (–±–µ–∑ r/)
            query: –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            limit: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Å—Ç–æ–≤
            time_filter: 'hour', 'day', 'week', 'month', 'year', 'all'
            sort: 'relevance', 'hot', 'top', 'new', 'comments'
            
        Returns:
            DataFrame —Å –ø–æ—Å—Ç–∞–º–∏
        """
        posts_data = []
        
        print(f"–ü–æ–∏—Å–∫ –≤ r/{subreddit_name}: '{query}'")
        print(f"–ü–µ—Ä–∏–æ–¥: {time_filter}, –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞: {sort}")
        
        try:
            subreddit = self.reddit.subreddit(subreddit_name)
            
            # –ü–æ–∏—Å–∫ –ø–æ—Å—Ç–æ–≤
            search_results = subreddit.search(
                query=query,
                limit=limit,
                time_filter=time_filter,
                sort=sort
            )
            
            for post in search_results:
                # –°–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ –æ –ø–æ—Å—Ç–µ
                post_info = {
                    'id': post.id,
                    'subreddit': str(post.subreddit),
                    'title': post.title,
                    'text': post.selftext,
                    'author': str(post.author) if post.author else '[deleted]',
                    'created_utc': datetime.fromtimestamp(post.created_utc),
                    'score': post.score,
                    'upvote_ratio': post.upvote_ratio,
                    'num_comments': post.num_comments,
                    'url': f"https://reddit.com{post.permalink}",
                    'is_self': post.is_self,
                    'link_flair_text': post.link_flair_text,
                    'engagement': post.score + post.num_comments,  # –ü—Ä–æ—Å—Ç–∞—è –º–µ—Ç—Ä–∏–∫–∞ engagement
                }
                
                posts_data.append(post_info)
            
            print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(posts_data)} –ø–æ—Å—Ç–æ–≤ –≤ r/{subreddit_name}")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≤ r/{subreddit_name}: {e}")
            return pd.DataFrame()
        
        return pd.DataFrame(posts_data)
    
    def search_multiple_subreddits(self, subreddits, query, limit_per_subreddit=100, time_filter='month'):
        """
        –ü–æ–∏—Å–∫ –ø–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º subreddits
        
        Args:
            subreddits: —Å–ø–∏—Å–æ–∫ –Ω–∞–∑–≤–∞–Ω–∏–π subreddits
            query: –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            limit_per_subreddit: –ª–∏–º–∏—Ç –ø–æ—Å—Ç–æ–≤ –Ω–∞ –∫–∞–∂–¥—ã–π subreddit
            time_filter: –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä
        """
        all_posts = []
        
        for subreddit in subreddits:
            print(f"\n  –ü–æ–∏—Å–∫ –≤ r/{subreddit}...")
            posts_df = self.search_subreddit(
                subreddit_name=subreddit,
                query=query,
                limit=limit_per_subreddit,
                time_filter=time_filter
            )
            
            if not posts_df.empty:
                all_posts.append(posts_df)
            
            # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ (rate limit)
            time.sleep(2)
        
        if not all_posts:
            return pd.DataFrame()
        
        combined = pd.concat(all_posts, ignore_index=True)
        
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ ID
        combined = combined.drop_duplicates(subset=['id'])
        
        return combined
    
    def get_top_posts(self, subreddit_name, time_filter='month', limit=50):
        """
        –ü–æ–ª—É—á–∏—Ç—å —Ç–æ–ø–æ–≤—ã–µ –ø–æ—Å—Ç—ã –∏–∑ subreddit
        
        –ü–æ–ª–µ–∑–Ω–æ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ç–µ–∫—É—â–∏—Ö —Ç—Ä–µ–Ω–¥–æ–≤ –∏ –ø—Ä–æ–±–ª–µ–º
        """
        posts_data = []
        
        print(f"–ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–æ–ø-–ø–æ—Å—Ç–æ–≤ –∏–∑ r/{subreddit_name} –∑–∞ {time_filter}")
        
        try:
            subreddit = self.reddit.subreddit(subreddit_name)
            
            for post in subreddit.top(time_filter=time_filter, limit=limit):
                post_info = {
                    'id': post.id,
                    'subreddit': str(post.subreddit),
                    'title': post.title,
                    'text': post.selftext,
                    'author': str(post.author) if post.author else '[deleted]',
                    'created_utc': datetime.fromtimestamp(post.created_utc),
                    'score': post.score,
                    'upvote_ratio': post.upvote_ratio,
                    'num_comments': post.num_comments,
                    'url': f"https://reddit.com{post.permalink}",
                    'engagement': post.score + post.num_comments,
                }
                
                posts_data.append(post_info)
            
            print(f"‚úÖ –ü–æ–ª—É—á–µ–Ω–æ {len(posts_data)} —Ç–æ–ø-–ø–æ—Å—Ç–æ–≤")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
            return pd.DataFrame()
        
        return pd.DataFrame(posts_data)
    
    def find_pain_points(self, posts_df):
        """
        –ê–Ω–∞–ª–∏–∑ –±–æ–ª–µ–≤—ã—Ö —Ç–æ—á–µ–∫ –≤ –ø–æ—Å—Ç–∞—Ö
        
        –ò—â–µ—Ç –ø–æ—Å—Ç—ã —Å –º–∞—Ä–∫–µ—Ä–∞–º–∏ –ø—Ä–æ–±–ª–µ–º –∏ —Ñ—Ä—É—Å—Ç—Ä–∞—Ü–∏–∏
        """
        if posts_df.empty:
            return pd.DataFrame()
        
        pain_keywords = [
            'struggling', 'frustrated', 'annoying', 'waste time', 'wasting time',
            'difficult', 'problem', 'issue', 'broken', 'hate', 'terrible',
            'wish', 'need', 'missing', 'slow', 'expensive', 'costly',
            'complicated', 'confusing', 'sucks', 'awful', 'pain',
            'nightmare', 'help', 'advice', 'how to', 'anyone know',
            'recommend', 'alternative', 'better than', 'tired of'
        ]
        
        pain_posts = []
        
        for _, post in posts_df.iterrows():
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º title –∏ text
            combined_text = f"{post['title']} {post['text']}".lower()
            
            matched_keywords = []
            for keyword in pain_keywords:
                if keyword in combined_text:
                    matched_keywords.append(keyword)
            
            if matched_keywords:
                pain_posts.append({
                    'post_id': post['id'],
                    'subreddit': post['subreddit'],
                    'title': post['title'],
                    'text': post['text'][:200] + '...' if len(post['text']) > 200 else post['text'],
                    'keywords': ', '.join(matched_keywords),
                    'engagement': post['engagement'],
                    'score': post['score'],
                    'num_comments': post['num_comments'],
                    'url': post['url']
                })
        
        return pd.DataFrame(pain_posts)
    
    def analyze_comments(self, post_id, limit=50):
        """
        –ê–Ω–∞–ª–∏–∑ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –∫ –ø–æ—Å—Ç—É
        
        –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ —á–∞—Å—Ç–æ —Å–æ–¥–µ—Ä–∂–∞—Ç –¥–µ—Ç–∞–ª—å–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º
        """
        comments_data = []
        
        try:
            submission = self.reddit.submission(id=post_id)
            submission.comments.replace_more(limit=0)  # –ù–µ –∑–∞–≥—Ä—É–∂–∞–µ–º "load more comments"
            
            for comment in submission.comments.list()[:limit]:
                if hasattr(comment, 'body'):
                    comments_data.append({
                        'comment_id': comment.id,
                        'author': str(comment.author) if comment.author else '[deleted]',
                        'text': comment.body,
                        'score': comment.score,
                        'created_utc': datetime.fromtimestamp(comment.created_utc),
                    })
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤: {e}")
        
        return pd.DataFrame(comments_data)
    
    def get_subreddit_info(self, subreddit_name):
        """
        –ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ subreddit
        
        –ü–æ–ª–µ–∑–Ω–æ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ä–∞–∑–º–µ—Ä–∞ –∞—É–¥–∏—Ç–æ—Ä–∏–∏
        """
        try:
            subreddit = self.reddit.subreddit(subreddit_name)
            
            info = {
                'name': subreddit.display_name,
                'title': subreddit.title,
                'description': subreddit.public_description,
                'subscribers': subreddit.subscribers,
                'active_users': subreddit.active_user_count,
                'created_utc': datetime.fromtimestamp(subreddit.created_utc),
                'url': f"https://reddit.com/r/{subreddit_name}"
            }
            
            return info
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
            return None
    
    def validate_saas_idea(self, idea_keywords, relevant_subreddits, output_file='reddit_validation.json'):
        """
        –ü–æ–ª–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è SaaS –∏–¥–µ–∏ —á–µ—Ä–µ–∑ Reddit
        
        Args:
            idea_keywords: —Å–ø–∏—Å–æ–∫ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –∏–¥–µ–µ–π
            relevant_subreddits: —Å–ø–∏—Å–æ–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö subreddits
            output_file: —Ñ–∞–π–ª –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            
        Returns:
            dict —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        """
        print(f"\n{'='*60}")
        print(f"Reddit –í–∞–ª–∏–¥–∞—Ü–∏—è SaaS –ò–¥–µ–∏")
        print(f"{'='*60}\n")
        
        validation_results = {
            'analysis_date': datetime.now().isoformat(),
            'idea_keywords': idea_keywords,
            'subreddits_analyzed': relevant_subreddits,
            'subreddit_stats': [],
            'posts_found': 0,
            'pain_points_found': 0,
            'top_posts': [],
            'pain_point_posts': [],
            'common_issues': {},
            'potential_competitors': [],
            'market_size_estimate': 0
        }
        
        # 1. –ê–Ω–∞–ª–∏–∑ subreddits
        print("üìä –ê–Ω–∞–ª–∏–∑ subreddits:")
        for subreddit in relevant_subreddits:
            info = self.get_subreddit_info(subreddit)
            if info:
                validation_results['subreddit_stats'].append(info)
                validation_results['market_size_estimate'] += info['subscribers']
                print(f"  r/{subreddit}: {info['subscribers']:,} –ø–æ–¥–ø–∏—Å—á–∏–∫–æ–≤, {info['active_users']:,} –∞–∫—Ç–∏–≤–Ω—ã—Ö")
        
        # 2. –ü–æ–∏—Å–∫ –ø–æ—Å—Ç–æ–≤ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        print("\nüîç –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –ø–æ—Å—Ç–æ–≤:")
        all_posts = []
        
        for keyword in idea_keywords:
            print(f"\n  –ö–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ: '{keyword}'")
            posts = self.search_multiple_subreddits(
                subreddits=relevant_subreddits,
                query=keyword,
                limit_per_subreddit=50,
                time_filter='month'
            )
            if not posts.empty:
                all_posts.append(posts)
            time.sleep(1)
        
        if all_posts:
            combined_posts = pd.concat(all_posts, ignore_index=True)
            combined_posts = combined_posts.drop_duplicates(subset=['id'])
            validation_results['posts_found'] = len(combined_posts)
            
            # 3. –ê–Ω–∞–ª–∏–∑ –±–æ–ª–µ–≤—ã—Ö —Ç–æ—á–µ–∫
            print("\nüî• –ê–Ω–∞–ª–∏–∑ –±–æ–ª–µ–≤—ã—Ö —Ç–æ—á–µ–∫:")
            pain_points = self.find_pain_points(combined_posts)
            validation_results['pain_points_found'] = len(pain_points)
            
            if not pain_points.empty:
                # –¢–æ–ø –ø–æ—Å—Ç–æ–≤ —Å –±–æ–ª–µ–≤—ã–º–∏ —Ç–æ—á–∫–∞–º–∏
                top_pain = pain_points.nlargest(10, 'engagement')
                validation_results['pain_point_posts'] = top_pain.to_dict('records')
                
                # –ê–Ω–∞–ª–∏–∑ —á–∞—Å—Ç—ã—Ö –ø—Ä–æ–±–ª–µ–º
                all_keywords = []
                for keywords_str in pain_points['keywords']:
                    all_keywords.extend(keywords_str.split(', '))
                keyword_counter = Counter(all_keywords)
                validation_results['common_issues'] = dict(keyword_counter.most_common(20))
                
                print(f"  –ù–∞–π–¥–µ–Ω–æ {len(pain_points)} –ø–æ—Å—Ç–æ–≤ —Å –±–æ–ª–µ–≤—ã–º–∏ —Ç–æ—á–∫–∞–º–∏")
                print(f"\n  –¢–æ–ø –ø—Ä–æ–±–ª–µ–º:")
                for issue, count in keyword_counter.most_common(10):
                    print(f"    '{issue}': {count} —É–ø–æ–º–∏–Ω–∞–Ω–∏–π")
            
            # 4. –¢–æ–ø –ø–æ—Å—Ç—ã –ø–æ engagement
            top_posts = combined_posts.nlargest(10, 'engagement')
            validation_results['top_posts'] = top_posts[[
                'title', 'subreddit', 'score', 'num_comments', 'url'
            ]].to_dict('records')
            
            # 5. –û—Ü–µ–Ω–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ –∏–¥–µ–∏ (scoring)
            score = 0
            reasons = []
            
            # –†–∞–∑–º–µ—Ä –∞—É–¥–∏—Ç–æ—Ä–∏–∏
            if validation_results['market_size_estimate'] > 100000:
                score += 25
                reasons.append("‚úÖ –ë–æ–ª—å—à–∞—è –∞—É–¥–∏—Ç–æ—Ä–∏—è (100k+ –ø–æ–¥–ø–∏—Å—á–∏–∫–æ–≤)")
            elif validation_results['market_size_estimate'] > 50000:
                score += 15
                reasons.append("‚úÖ –°—Ä–µ–¥–Ω—è—è –∞—É–¥–∏—Ç–æ—Ä–∏—è (50k+ –ø–æ–¥–ø–∏—Å—á–∏–∫–æ–≤)")
            
            # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Å—Ç–æ–≤
            if validation_results['posts_found'] > 50:
                score += 20
                reasons.append("‚úÖ –ú–Ω–æ–≥–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –ø–æ—Å—Ç–æ–≤ (50+)")
            elif validation_results['posts_found'] > 20:
                score += 10
                reasons.append("‚úÖ –ï—Å—Ç—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –ø–æ—Å—Ç—ã (20+)")
            
            # –ë–æ–ª–µ–≤—ã–µ —Ç–æ—á–∫–∏
            if validation_results['pain_points_found'] > 20:
                score += 30
                reasons.append("‚úÖ –ú–Ω–æ–≥–æ –±–æ–ª–µ–≤—ã—Ö —Ç–æ—á–µ–∫ (20+)")
            elif validation_results['pain_points_found'] > 10:
                score += 20
                reasons.append("‚úÖ –ï—Å—Ç—å –±–æ–ª–µ–≤—ã–µ —Ç–æ—á–∫–∏ (10+)")
            elif validation_results['pain_points_found'] > 5:
                score += 10
                reasons.append("‚ö†Ô∏è –ù–µ–º–Ω–æ–≥–æ –±–æ–ª–µ–≤—ã—Ö —Ç–æ—á–µ–∫ (5+)")
            
            # Engagement
            avg_engagement = combined_posts['engagement'].mean()
            if avg_engagement > 100:
                score += 15
                reasons.append(f"‚úÖ –í—ã—Å–æ–∫–∏–π engagement (avg {avg_engagement:.0f})")
            elif avg_engagement > 50:
                score += 10
                reasons.append(f"‚úÖ –°—Ä–µ–¥–Ω–∏–π engagement (avg {avg_engagement:.0f})")
            
            # –°–≤–µ–∂–µ—Å—Ç—å –ø—Ä–æ–±–ª–µ–º—ã (–ø–æ—Å—Ç—ã –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–π –º–µ—Å—è—Ü)
            recent_posts = combined_posts[
                combined_posts['created_utc'] > datetime.now() - timedelta(days=30)
            ]
            if len(recent_posts) > 20:
                score += 10
                reasons.append("‚úÖ –ü—Ä–æ–±–ª–µ–º–∞ –∞–∫—Ç—É–∞–ª—å–Ω–∞ (–º–Ω–æ–≥–æ —Å–≤–µ–∂–∏—Ö –ø–æ—Å—Ç–æ–≤)")
            
            validation_results['validation_score'] = score
            validation_results['score_reasons'] = reasons
            
            # –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è score
            if score >= 80:
                verdict = "üöÄ –û–¢–õ–ò–ß–ù–ê–Ø –ò–î–ï–Ø - –°–∏–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è"
            elif score >= 60:
                verdict = "‚úÖ –•–û–†–û–®–ê–Ø –ò–î–ï–Ø - –ï—Å—Ç—å –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª"
            elif score >= 40:
                verdict = "‚ö†Ô∏è –°–†–ï–î–ù–Ø–Ø –ò–î–ï–Ø - –ù—É–∂–Ω—ã –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è"
            else:
                verdict = "‚ùå –°–õ–ê–ë–ê–Ø –í–ê–õ–ò–î–ê–¶–ò–Ø - –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è pivot"
            
            validation_results['verdict'] = verdict
            
        else:
            validation_results['verdict'] = "‚ùå –ù–ï–¢ –î–ê–ù–ù–´–• - –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –ø–æ—Å—Ç–æ–≤"
            validation_results['validation_score'] = 0
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(validation_results, f, indent=2, ensure_ascii=False, default=str)
        
        # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        print(f"\n{'='*60}")
        print(f"üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –í–ê–õ–ò–î–ê–¶–ò–ò")
        print(f"{'='*60}")
        print(f"\nüéØ –û—Ü–µ–Ω–∫–∞: {validation_results['validation_score']}/100")
        print(f"üìã –í–µ—Ä–¥–∏–∫—Ç: {validation_results['verdict']}")
        print(f"\nüìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        print(f"  - –†–∞–∑–º–µ—Ä –∞—É–¥–∏—Ç–æ—Ä–∏–∏: {validation_results['market_size_estimate']:,} –ø–æ–¥–ø–∏—Å—á–∏–∫–æ–≤")
        print(f"  - –ù–∞–π–¥–µ–Ω–æ –ø–æ—Å—Ç–æ–≤: {validation_results['posts_found']}")
        print(f"  - –ë–æ–ª–µ–≤—ã—Ö —Ç–æ—á–µ–∫: {validation_results['pain_points_found']}")
        
        if 'score_reasons' in validation_results:
            print(f"\nüí° –ü–æ—á–µ–º—É —ç—Ç–∞ –æ—Ü–µ–Ω–∫–∞:")
            for reason in validation_results['score_reasons']:
                print(f"  {reason}")
        
        print(f"\n‚úÖ –ü–æ–ª–Ω—ã–π –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_file}")
        
        return validation_results


class RedditAdvancedSearch:
    """
    –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –ø–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã –∏ helper –º–µ—Ç–æ–¥—ã –¥–ª—è Reddit
    """
    
    @staticmethod
    def get_saas_subreddits():
        """
        –°–ø–∏—Å–æ–∫ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö subreddits –¥–ª—è SaaS –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        """
        return {
            'general': ['SaaS', 'Entrepreneur', 'startups', 'smallbusiness'],
            'tech': ['webdev', 'programming', 'devops', 'sysadmin'],
            'marketing': ['marketing', 'digitalmarketing', 'SEO', 'PPC'],
            'productivity': ['productivity', 'gtd', 'organization'],
            'design': ['web_design', 'UI_Design', 'UXDesign'],
            'freelance': ['freelance', 'forhire', 'freelance_forhire']
        }
    
    @staticmethod
    def build_pain_query(topic):
        """
        –ü–æ—Å—Ç—Ä–æ–∏—Ç—å –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–∏—Å–∫–∞ –±–æ–ª–µ–≤—ã—Ö —Ç–æ—á–µ–∫
        
        Reddit search –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç boolean –æ–ø–µ—Ä–∞—Ç–æ—Ä—ã
        """
        pain_keywords = [
            'struggling', 'frustrated', 'problem', 'issue',
            'help', 'advice', 'how to', 'difficult'
        ]
        
        # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º topic —Å pain keywords
        queries = [f'"{topic}" "{keyword}"' for keyword in pain_keywords]
        
        return queries
    
    @staticmethod
    def build_solution_query(topic):
        """
        –ü–æ—Å—Ç—Ä–æ–∏—Ç—å –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–∏—Å–∫–∞ –∑–∞–ø—Ä–æ—Å–æ–≤ –Ω–∞ —Ä–µ—à–µ–Ω–∏–µ
        """
        solution_keywords = [
            'recommend', 'best tool', 'looking for',
            'need help', 'what do you use', 'alternative to'
        ]
        
        queries = [f'"{topic}" "{keyword}"' for keyword in solution_keywords]
        
        return queries
    
    @staticmethod
    def build_competitor_query(competitors):
        """
        –ü–æ—Å—Ç—Ä–æ–∏—Ç—å –∑–∞–ø—Ä–æ—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤
        """
        queries = []
        for competitor in competitors:
            queries.append(f'"{competitor}"')
            queries.append(f'"{competitor}" alternative')
            queries.append(f'"{competitor}" vs')
        
        return queries


def main():
    """
    –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è Reddit scraper
    """
    
    # –í–ê–ñ–ù–û: –ó–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ –≤–∞—à–∏ credentials
    CLIENT_ID = "–≤–∞—à_client_id"
    CLIENT_SECRET = "–≤–∞—à_client_secret"
    USER_AGENT = "SaaS Validator 1.0 by u/yourname"
    
    scraper = RedditSaaSValidator(
        client_id=CLIENT_ID,
        client_secret=CLIENT_SECRET,
        user_agent=USER_AGENT
    )
    
    # –ü—Ä–∏–º–µ—Ä 1: –ü—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫
    print("=" * 60)
    print("–ü–†–ò–ú–ï–† 1: –ü–æ–∏—Å–∫ –≤ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º subreddit")
    print("=" * 60)
    
    posts = scraper.search_subreddit(
        subreddit_name='SaaS',
        query='email marketing',
        limit=50,
        time_filter='month'
    )
    
    if not posts.empty:
        posts.to_csv('reddit_posts.csv', index=False, encoding='utf-8')
        print(f"\n‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ reddit_posts.csv")
    
    # –ü—Ä–∏–º–µ—Ä 2: –ü–æ–∏—Å–∫ –ø–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º subreddits
    print("\n" + "=" * 60)
    print("–ü–†–ò–ú–ï–† 2: –ü–æ–∏—Å–∫ –ø–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º subreddits")
    print("=" * 60)
    
    subreddits = ['SaaS', 'Entrepreneur', 'startups', 'smallbusiness']
    
    all_posts = scraper.search_multiple_subreddits(
        subreddits=subreddits,
        query='cold email',
        limit_per_subreddit=30,
        time_filter='month'
    )
    
    if not all_posts.empty:
        # –ê–Ω–∞–ª–∏–∑ –±–æ–ª–µ–≤—ã—Ö —Ç–æ—á–µ–∫
        pain_points = scraper.find_pain_points(all_posts)
        
        if not pain_points.empty:
            pain_points.to_csv('reddit_pain_points.csv', index=False)
            print(f"\n‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(pain_points)} –ø–æ—Å—Ç–æ–≤ —Å –±–æ–ª–µ–≤—ã–º–∏ —Ç–æ—á–∫–∞–º–∏")
    
    # –ü—Ä–∏–º–µ—Ä 3: –ü–æ–ª–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –∏–¥–µ–∏
    print("\n" + "=" * 60)
    print("–ü–†–ò–ú–ï–† 3: –ü–æ–ª–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è SaaS –∏–¥–µ–∏")
    print("=" * 60)
    
    idea_keywords = [
        'email automation',
        'cold email tool',
        'email outreach',
        'email marketing software'
    ]
    
    relevant_subreddits = ['SaaS', 'Entrepreneur', 'marketing', 'sales']
    
    validation = scraper.validate_saas_idea(
        idea_keywords=idea_keywords,
        relevant_subreddits=relevant_subreddits,
        output_file='reddit_validation.json'
    )
    
    # –ü—Ä–∏–º–µ—Ä 4: –ê–Ω–∞–ª–∏–∑ —Ç–æ–ø-–ø–æ—Å—Ç–æ–≤ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ç—Ä–µ–Ω–¥–æ–≤
    print("\n" + "=" * 60)
    print("–ü–†–ò–ú–ï–† 4: –ê–Ω–∞–ª–∏–∑ —Ç–æ–ø-–ø–æ—Å—Ç–æ–≤")
    print("=" * 60)
    
    top_posts = scraper.get_top_posts(
        subreddit_name='SaaS',
        time_filter='month',
        limit=20
    )
    
    if not top_posts.empty:
        print("\n–¢–æ–ø-5 –ø–æ—Å—Ç–æ–≤ –∑–∞ –º–µ—Å—è—Ü:")
        for _, post in top_posts.head(5).iterrows():
            print(f"\n  üìå {post['title']}")
            print(f"     ‚¨ÜÔ∏è {post['score']} | üí¨ {post['num_comments']} | üîó {post['url']}")
    
    # –ü—Ä–∏–º–µ—Ä 5: –ê–Ω–∞–ª–∏–∑ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤
    print("\n" + "=" * 60)
    print("–ü–†–ò–ú–ï–† 5: –ê–Ω–∞–ª–∏–∑ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤")
    print("=" * 60)
    
    competitors = ['mailchimp', 'sendgrid', 'convertkit']
    competitor_queries = RedditAdvancedSearch.build_competitor_query(competitors)
    
    for query in competitor_queries[:3]:  # –ü–µ—Ä–≤—ã–µ 3 –∑–∞–ø—Ä–æ—Å–∞
        print(f"\n–ó–∞–ø—Ä–æ—Å: {query}")
        posts = scraper.search_subreddit(
            subreddit_name='marketing',
            query=query,
            limit=20,
            time_filter='month'
        )
        
        if not posts.empty:
            print(f"  –ù–∞–π–¥–µ–Ω–æ {len(posts)} –ø–æ—Å—Ç–æ–≤")


if __name__ == "__main__":
    main()
