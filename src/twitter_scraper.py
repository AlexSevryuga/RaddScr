"""
Twitter/X Scraper –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ SaaS –∏–¥–µ–π

–°–æ–±–∏—Ä–∞–µ—Ç —Ç–≤–∏—Ç—ã, –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç engagement –∏ –Ω–∞—Ö–æ–¥–∏—Ç –ø—Ä–æ–±–ª–µ–º—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
"""

import tweepy
import pandas as pd
from datetime import datetime, timedelta
import json
import re
from collections import Counter
import time


class TwitterSaaSValidator:
    def __init__(self, bearer_token):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Twitter API v2 –∫–ª–∏–µ–Ω—Ç–∞
        
        –î–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è bearer_token:
        1. –ü–µ—Ä–µ–π–¥–∏—Ç–µ –Ω–∞ https://developer.twitter.com/en/portal/dashboard
        2. –°–æ–∑–¥–∞–π—Ç–µ –Ω–æ–≤–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ
        3. –ü–æ–ª—É—á–∏—Ç–µ Bearer Token –∏–∑ —Ä–∞–∑–¥–µ–ª–∞ "Keys and tokens"
        """
        self.client = tweepy.Client(bearer_token=bearer_token)
    
    def search_tweets(self, query, max_results=100, days_back=7):
        """
        –ü–æ–∏—Å–∫ —Ç–≤–∏—Ç–æ–≤ –ø–æ –∑–∞–ø—Ä–æ—Å—É
        
        Args:
            query: –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å (–º–æ–∂–µ—Ç –≤–∫–ª—é—á–∞—Ç—å –æ–ø–µ—Ä–∞—Ç–æ—Ä—ã)
            max_results: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–≤–∏—Ç–æ–≤ (10-100 –∑–∞ –∑–∞–ø—Ä–æ—Å)
            days_back: —Å–∫–æ–ª—å–∫–æ –¥–Ω–µ–π –Ω–∞–∑–∞–¥ –∏—Å–∫–∞—Ç—å
            
        Returns:
            DataFrame —Å —Ç–≤–∏—Ç–∞–º–∏
        """
        tweets_data = []
        
        # –í—Ä–µ–º–µ–Ω–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä
        start_time = datetime.utcnow() - timedelta(days=days_back)
        
        print(f"–ü–æ–∏—Å–∫ —Ç–≤–∏—Ç–æ–≤ –ø–æ –∑–∞–ø—Ä–æ—Å—É: {query}")
        print(f"–ü–µ—Ä–∏–æ–¥: –ø–æ—Å–ª–µ–¥–Ω–∏–µ {days_back} –¥–Ω–µ–π")
        
        try:
            # –ü–æ–∏—Å–∫ —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏
            tweets = self.client.search_recent_tweets(
                query=query,
                max_results=max_results,
                start_time=start_time,
                tweet_fields=['created_at', 'public_metrics', 'author_id', 'lang'],
                expansions=['author_id'],
                user_fields=['username', 'name', 'public_metrics']
            )
            
            if not tweets.data:
                print("–¢–≤–∏—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
                return pd.DataFrame()
            
            # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
            users = {user.id: user for user in tweets.includes.get('users', [])}
            
            for tweet in tweets.data:
                user = users.get(tweet.author_id)
                
                tweet_info = {
                    'id': tweet.id,
                    'text': tweet.text,
                    'created_at': tweet.created_at,
                    'lang': tweet.lang,
                    'likes': tweet.public_metrics['like_count'],
                    'retweets': tweet.public_metrics['retweet_count'],
                    'replies': tweet.public_metrics['reply_count'],
                    'impressions': tweet.public_metrics.get('impression_count', 0),
                    'engagement': tweet.public_metrics['like_count'] + 
                                 tweet.public_metrics['retweet_count'] + 
                                 tweet.public_metrics['reply_count'],
                    'author_username': user.username if user else None,
                    'author_name': user.name if user else None,
                    'author_followers': user.public_metrics['followers_count'] if user else 0,
                    'url': f"https://twitter.com/i/web/status/{tweet.id}"
                }
                
                tweets_data.append(tweet_info)
            
            print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(tweets_data)} —Ç–≤–∏—Ç–æ–≤")
            
        except tweepy.errors.TweepyException as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ Twitter API: {e}")
            return pd.DataFrame()
        
        return pd.DataFrame(tweets_data)
    
    def search_multiple_keywords(self, keywords, max_results_per_keyword=50, days_back=7):
        """
        –ü–æ–∏—Å–∫ –ø–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        """
        all_tweets = []
        
        for keyword in keywords:
            print(f"  –ü–æ–∏—Å–∫: {keyword}")
            tweets_df = self.search_tweets(
                query=keyword,
                max_results=max_results_per_keyword,
                days_back=days_back
            )
            
            if not tweets_df.empty:
                tweets_df['keyword'] = keyword
                all_tweets.append(tweets_df)
            
            # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ (rate limit)
            time.sleep(2)
        
        if not all_tweets:
            return pd.DataFrame()
        
        combined = pd.concat(all_tweets, ignore_index=True)
        
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ ID
        combined = combined.drop_duplicates(subset=['id'])
        
        return combined
    
    def find_pain_points(self, tweets_df):
        """
        –ê–Ω–∞–ª–∏–∑ –±–æ–ª–µ–≤—ã—Ö —Ç–æ—á–µ–∫ –≤ —Ç–≤–∏—Ç–∞—Ö
        """
        if tweets_df.empty:
            return pd.DataFrame()
        
        pain_keywords = [
            'struggling', 'frustrated', 'annoying', 'waste time',
            'difficult', 'problem', 'issue', 'broken', 'hate',
            'wish', 'need', 'missing', 'slow', 'expensive',
            'complicated', 'confusing'
        ]
        
        pain_tweets = []
        
        for _, tweet in tweets_df.iterrows():
            text_lower = tweet['text'].lower()
            
            for keyword in pain_keywords:
                if keyword in text_lower:
                    pain_tweets.append({
                        'tweet_id': tweet['id'],
                        'text': tweet['text'],
                        'keyword': keyword,
                        'engagement': tweet['engagement'],
                        'created_at': tweet['created_at'],
                        'url': tweet['url']
                    })
                    break  # –û–¥–∏–Ω —Ç–≤–∏—Ç —Å—á–∏—Ç–∞–µ–º —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω —Ä–∞–∑
        
        return pd.DataFrame(pain_tweets)
    
    def analyze_hashtags(self, tweets_df):
        """
        –ê–Ω–∞–ª–∏–∑ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö —Ö–µ—à—Ç–µ–≥–æ–≤
        """
        if tweets_df.empty:
            return Counter()
        
        hashtags = []
        for text in tweets_df['text']:
            found_hashtags = re.findall(r'#(\w+)', text)
            hashtags.extend([h.lower() for h in found_hashtags])
        
        return Counter(hashtags)
    
    def analyze_mentions(self, tweets_df):
        """
        –ê–Ω–∞–ª–∏–∑ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π (@mentions) - —á–∞—Å—Ç–æ —ç—Ç–æ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç—ã
        """
        if tweets_df.empty:
            return Counter()
        
        mentions = []
        for text in tweets_df['text']:
            found_mentions = re.findall(r'@(\w+)', text)
            mentions.extend([m.lower() for m in found_mentions])
        
        return Counter(mentions)
    
    def get_user_tweets(self, username, max_results=100):
        """
        –ü–æ–ª—É—á–∏—Ç—å —Ç–≤–∏—Ç—ã –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        
        –ü–æ–ª–µ–∑–Ω–æ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ –∏–ª–∏ thought leaders
        """
        try:
            user = self.client.get_user(username=username)
            
            if not user.data:
                print(f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å @{username} –Ω–µ –Ω–∞–π–¥–µ–Ω")
                return pd.DataFrame()
            
            user_id = user.data.id
            
            tweets = self.client.get_users_tweets(
                id=user_id,
                max_results=max_results,
                tweet_fields=['created_at', 'public_metrics']
            )
            
            if not tweets.data:
                return pd.DataFrame()
            
            tweets_data = []
            for tweet in tweets.data:
                tweets_data.append({
                    'text': tweet.text,
                    'created_at': tweet.created_at,
                    'likes': tweet.public_metrics['like_count'],
                    'retweets': tweet.public_metrics['retweet_count'],
                    'replies': tweet.public_metrics['reply_count'],
                    'engagement': tweet.public_metrics['like_count'] + 
                                 tweet.public_metrics['retweet_count'] + 
                                 tweet.public_metrics['reply_count']
                })
            
            return pd.DataFrame(tweets_data)
            
        except tweepy.errors.TweepyException as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
            return pd.DataFrame()
    
    def generate_report(self, keywords, output_file='twitter_analysis.json'):
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø–æ–ª–Ω—ã–π –æ—Ç—á–µ—Ç –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∏–¥–µ–∏
        """
        print(f"\n{'='*60}")
        print(f"Twitter/X –ê–Ω–∞–ª–∏–∑")
        print(f"{'='*60}\n")
        
        # –°–æ–±–∏—Ä–∞–µ–º —Ç–≤–∏—Ç—ã
        tweets_df = self.search_multiple_keywords(keywords, max_results_per_keyword=100)
        
        if tweets_df.empty:
            print("‚ùå –¢–≤–∏—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            return None, None
        
        # –ê–Ω–∞–ª–∏–∑
        pain_points = self.find_pain_points(tweets_df)
        hashtags = self.analyze_hashtags(tweets_df)
        mentions = self.analyze_mentions(tweets_df)
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç
        report = {
            'platform': 'Twitter/X',
            'analysis_date': datetime.now().isoformat(),
            'total_tweets': len(tweets_df),
            'keywords_searched': keywords,
            'top_tweets': tweets_df.nlargest(10, 'engagement')[
                ['text', 'engagement', 'likes', 'retweets', 'url']
            ].to_dict('records'),
            'pain_points_count': len(pain_points),
            'top_pain_keywords': pain_points['keyword'].value_counts().head(10).to_dict() if len(pain_points) > 0 else {},
            'top_hashtags': dict(hashtags.most_common(20)),
            'top_mentions': dict(mentions.most_common(20)),
            'engagement_stats': {
                'avg_likes': float(tweets_df['likes'].mean()),
                'avg_retweets': float(tweets_df['retweets'].mean()),
                'avg_replies': float(tweets_df['replies'].mean()),
                'total_engagement': int(tweets_df['engagement'].sum())
            },
            'language_distribution': tweets_df['lang'].value_counts().to_dict()
        }
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"\n‚úÖ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_file}")
        
        # –í—ã–≤–æ–¥–∏–º –∫—Ä–∞—Ç–∫—É—é —Å–≤–æ–¥–∫—É
        print(f"\nüìä –ö—Ä–∞—Ç–∫–∞—è —Å–≤–æ–¥–∫–∞:")
        print(f"- –ù–∞–π–¥–µ–Ω–æ —Ç–≤–∏—Ç–æ–≤: {len(tweets_df)}")
        print(f"- Pain points: {len(pain_points)}")
        print(f"- –°—Ä–µ–¥–Ω–∏–π engagement: {report['engagement_stats']['avg_likes']:.1f} likes")
        print(f"- –û–±—â–∏–π engagement: {report['engagement_stats']['total_engagement']}")
        
        if hashtags:
            print(f"\nüî• –¢–æ–ø —Ö–µ—à—Ç–µ–≥–æ–≤:")
            for tag, count in hashtags.most_common(10):
                print(f"  #{tag}: {count}")
        
        if mentions:
            print(f"\nüë§ –¢–æ–ø —É–ø–æ–º–∏–Ω–∞–Ω–∏–π (–≤–æ–∑–º–æ–∂–Ω—ã–µ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç—ã):")
            for mention, count in mentions.most_common(10):
                print(f"  @{mention}: {count}")
        
        return report, tweets_df


class TwitterAdvancedSearch:
    """
    –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –ø–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è Twitter
    """
    
    @staticmethod
    def build_pain_query(topic):
        """
        –°—Ç—Ä–æ–∏—Ç –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–∏—Å–∫–∞ –±–æ–ª–µ–≤—ã—Ö —Ç–æ—á–µ–∫
        """
        pain_words = ['struggling', 'frustrated', 'annoying', 'hate', 'problem', 'issue']
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º OR –æ–ø–µ—Ä–∞—Ç–æ—Ä
        pain_query = f'{topic} ({" OR ".join(pain_words)})'
        return pain_query
    
    @staticmethod
    def build_solution_query(topic):
        """
        –°—Ç—Ä–æ–∏—Ç –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–∏—Å–∫–∞ –∑–∞–ø—Ä–æ—Å–æ–≤ –Ω–∞ —Ä–µ—à–µ–Ω–∏–µ
        """
        solution_words = ['how to', 'best way', 'recommend', 'looking for', 'need help', 'advice']
        solution_query = f'{topic} ({" OR ".join(solution_words)})'
        return solution_query
    
    @staticmethod
    def build_competitor_query(topic, competitors):
        """
        –°—Ç—Ä–æ–∏—Ç –∑–∞–ø—Ä–æ—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤
        """
        # –ò—â–µ–º —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ —Å –º–Ω–µ–Ω–∏—è–º–∏
        competitor_mentions = ' OR '.join([f'@{comp}' for comp in competitors])
        query = f'{topic} ({competitor_mentions})'
        return query
    
    @staticmethod
    def build_willingness_to_pay_query(topic):
        """
        –°—Ç—Ä–æ–∏—Ç –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–∏—Å–∫–∞ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ –ø–ª–∞—Ç–∏—Ç—å
        """
        payment_words = ['worth it', 'price', 'expensive', 'cheap', 'paying for', 'subscription']
        query = f'{topic} ({" OR ".join(payment_words)})'
        return query


def main():
    """
    –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è Twitter scraper
    """
    
    # –í–ê–ñ–ù–û: –ó–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ –≤–∞—à Bearer Token
    BEARER_TOKEN = "–≤–∞—à_bearer_token"
    
    scraper = TwitterSaaSValidator(BEARER_TOKEN)
    
    # –ü—Ä–∏–º–µ—Ä 1: –ü—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫
    print("=" * 60)
    print("–ü–†–ò–ú–ï–† 1: –ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º")
    print("=" * 60)
    
    keywords = [
        'email marketing tool',
        'email automation',
        'newsletter platform',
        'cold email software'
    ]
    
    report, tweets = scraper.generate_report(keywords)
    
    if tweets is not None and not tweets.empty:
        tweets.to_csv('twitter_tweets.csv', index=False, encoding='utf-8')
    
    # –ü—Ä–∏–º–µ—Ä 2: –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –±–æ–ª–µ–≤—ã—Ö —Ç–æ—á–µ–∫
    print("\n" + "=" * 60)
    print("–ü–†–ò–ú–ï–† 2: –ü–æ–∏—Å–∫ –±–æ–ª–µ–≤—ã—Ö —Ç–æ—á–µ–∫")
    print("=" * 60)
    
    topic = "team collaboration"
    pain_query = TwitterAdvancedSearch.build_pain_query(topic)
    
    pain_tweets = scraper.search_tweets(pain_query, max_results=100, days_back=7)
    
    if not pain_tweets.empty:
        pain_tweets.to_csv('twitter_pain_points.csv', index=False)
        print(f"\n‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(pain_tweets)} —Ç–≤–∏—Ç–æ–≤ —Å –±–æ–ª–µ–≤—ã–º–∏ —Ç–æ—á–∫–∞–º–∏")
    
    # –ü—Ä–∏–º–µ—Ä 3: –ê–Ω–∞–ª–∏–∑ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤
    print("\n" + "=" * 60)
    print("–ü–†–ò–ú–ï–† 3: –ê–Ω–∞–ª–∏–∑ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤")
    print("=" * 60)
    
    competitors = ['notion', 'airtable', 'asana', 'monday']
    competitor_query = TwitterAdvancedSearch.build_competitor_query('productivity', competitors)
    
    competitor_tweets = scraper.search_tweets(competitor_query, max_results=100)
    
    if not competitor_tweets.empty:
        competitor_tweets.to_csv('twitter_competitors.csv', index=False)
        
        # –ê–Ω–∞–ª–∏–∑ sentiment –∫ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞–º
        mentions = scraper.analyze_mentions(competitor_tweets)
        print("\n–£–ø–æ–º–∏–Ω–∞–Ω–∏—è –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤:")
        for mention, count in mentions.most_common(10):
            print(f"  @{mention}: {count} —Ä–∞–∑")
    
    # –ü—Ä–∏–º–µ—Ä 4: –ê–Ω–∞–ª–∏–∑ thought leaders
    print("\n" + "=" * 60)
    print("–ü–†–ò–ú–ï–† 4: –ê–Ω–∞–ª–∏–∑ thought leaders")
    print("=" * 60)
    
    thought_leaders = ['naval', 'levelsio', 'paulg', 'patio11']
    
    for leader in thought_leaders:
        print(f"\n–ê–Ω–∞–ª–∏–∑ @{leader}...")
        user_tweets = scraper.get_user_tweets(leader, max_results=50)
        
        if not user_tweets.empty:
            avg_engagement = user_tweets['engagement'].mean()
            print(f"  –°—Ä–µ–¥–Ω–∏–π engagement: {avg_engagement:.1f}")
            print(f"  –¢–æ–ø —Ç–≤–∏—Ç: {user_tweets.nlargest(1, 'engagement')['text'].values[0][:100]}...")


if __name__ == "__main__":
    main()
